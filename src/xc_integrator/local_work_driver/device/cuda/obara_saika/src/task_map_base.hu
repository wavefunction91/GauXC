#include "device_specific/cuda_device_constants.hpp"
#include "../../cuda_aos_scheme1.hpp"
#include <tuple>
#include <cuda/std/tuple>

namespace XGPU {

namespace constants {
  constexpr double sqrt_3 = 1.7320508075688772;
}

namespace sph {

__inline__ __device__ auto tform_l2(
  double xx, double xy, double xz, double yy, double yz, double zz
) {

  double m2 = constants::sqrt_3 * xy;
  double m1 = constants::sqrt_3 * yz;
  double z0 = zz - 0.5 * (xx + yy);
  double p1 = constants::sqrt_3 * xz;
  double p2 = constants::sqrt_3 * 0.5 * (xx - yy);

  return cuda::std::make_tuple(m2, m1, z0, p1, p2);

}

__inline__ __device__ auto itform_l2(
  double m2, double m1, double z0, double p1, double p2
) {

  double xx = 0.5 * (-z0 + constants::sqrt_3 * p2);
  double xy = constants::sqrt_3 * m2;
  double xz = constants::sqrt_3 * p1;
  double yy = -0.5 * (z0 + constants::sqrt_3 * p2);
  double yz = constants::sqrt_3 * m1;
  double zz = z0;

  return cuda::std::make_tuple(xx,xy,xz,yy,yz,zz);

}

}

using namespace GauXC;


template<typename T>
__inline__ __device__ void load_primpair_shared(
  const int laneId, const int warpId, const int n,
  const T* src_t, T* dst_t) {

  const int32_t* src = (const int32_t*) src_t;
  int32_t* dst = (int32_t*) dst_t;
  const int num_transfers = n * sizeof(GauXC::PrimitivePair<double>) / sizeof(int32_t);

  for (int i = laneId; i < num_transfers; i += GauXC::cuda::warp_size) {
    dst[i] = src[i]; 
  }
}

enum class ObaraSaikaType {
  base,
  swap,
  diag
};

template<ObaraSaikaType type>
struct ObaraSaikaBaseParams {
  const double *Xi;
  const double *Xj;
  double *Gi;
  double *Gj;

  __inline__ __device__ ObaraSaikaBaseParams( 
    const double *Xi_, const double *Xj_,
    double *Gi_, double *Gj_,
    double* sp_X_AB_device,
    double* sp_Y_AB_device,
    double* sp_Z_AB_device,
    const int index) {
    if constexpr (type == ObaraSaikaType::diag) {
      Xi = Xi_;
      Xj = Xi_;
      Gi = Gi_;
      Gj = Gi_;
    } else if constexpr (type == ObaraSaikaType::swap) {
      Xi = Xj_;
      Xj = Xi_;
      Gi = Gj_;
      Gj = Gi_;
    } else {
      Xi = Xi_;
      Xj = Xj_;
      Gi = Gi_;
      Gj = Gj_;
    }
  }
};

template<ObaraSaikaType type>
struct ObaraSaikaParamsWithAB : ObaraSaikaBaseParams<type> {
  double X_AB;
  double Y_AB;
  double Z_AB;

  __inline__ __device__ ObaraSaikaParamsWithAB( 
    const double *Xi_, const double *Xj_,
    double *Gi_, double *Gj_,
    double* sp_X_AB_device,
    double* sp_Y_AB_device,
    double* sp_Z_AB_device,
    const int index) 
    : ObaraSaikaBaseParams<type>(
        Xi_, Xj_, Gi_, Gj_, 
        sp_X_AB_device, sp_Y_AB_device, sp_Z_AB_device, 
        index) {

    if constexpr (type == ObaraSaikaType::swap) {
      X_AB = -1.0 * sp_X_AB_device[index];
      Y_AB = -1.0 * sp_Y_AB_device[index];
      Z_AB = -1.0 * sp_Z_AB_device[index];
    } else {
      X_AB = sp_X_AB_device[index];
      Y_AB = sp_Y_AB_device[index];
      Z_AB = sp_Z_AB_device[index];
    }
  }
};


template<typename AngularMomentum>
__global__
__launch_bounds__(AngularMomentum::num_threads, 1)
void task_map_kernel(
  int ntask, int nsubtask,
  GauXC::XCDeviceTask*                device_tasks,
  const GauXC::TaskToShellPairDevice* task2sp,
  const int4* subtasks,
  const int32_t* nprim_pairs_device,
  GauXC::PrimitivePair<double>** prim_pair_ptr_device,
  double* sp_X_AB_device,
  double* sp_Y_AB_device,
  double* sp_Z_AB_device,
  double *boys_table) {

  static constexpr int points_per_subtask = AngularMomentum::points_per_subtask;
  static constexpr int num_warps = AngularMomentum::num_warps;

  __shared__ double4 s_task_data[points_per_subtask];

  const int warpId = threadIdx.x / GauXC::cuda::warp_size;
  
  const int i_subtask = blockIdx.x;
  const int i_task = subtasks[i_subtask].x;
  const int point_start = subtasks[i_subtask].y;
  const int point_end = subtasks[i_subtask].z;
  const int point_count = point_end - point_start;

  const auto* task = device_tasks + i_task;

  const int npts = task->npts;

  const auto* points_x = task->points_x;
  const auto* points_y = task->points_y;
  const auto* points_z = task->points_z;
  const auto* weights = task->weights;

  const auto nsp = task2sp[i_task].nsp;

  // NOTE: util::div_ceil converts to 64bit int
  const int npts_block = util::div_ceil(point_count, blockDim.x);

  for (int i_block = 0; i_block < npts_block; i_block++) {
    const int i = point_start + i_block * blockDim.x;

    // load point into registers
    const double point_x = points_x[i + threadIdx.x];
    const double point_y = points_y[i + threadIdx.x];
    const double point_z = points_z[i + threadIdx.x];
    const double weight = weights[i + threadIdx.x];

    s_task_data[threadIdx.x].x = point_x;
    s_task_data[threadIdx.x].y = point_y;
    s_task_data[threadIdx.x].z = point_z;
    s_task_data[threadIdx.x].w = weight;
    __syncthreads();

    for (int j = num_warps*blockIdx.y+warpId; j < nsp; j+=num_warps*gridDim.y) {
      const auto i_off = task2sp[i_task].task_shell_off_row_device[j];
      const auto j_off = task2sp[i_task].task_shell_off_col_device[j];

      const auto index =  task2sp[i_task].shell_pair_linear_idx_device[j];
      const auto* pp = prim_pair_ptr_device[index];
      const auto nprim_pairs = nprim_pairs_device[index];

      const auto param = AngularMomentum::Params(
        task->fmat + i_off + i,
        task->fmat + j_off + i,
        task->gmat + i_off + i,
        task->gmat + j_off + i,
        sp_X_AB_device,
        sp_Y_AB_device,
        sp_Z_AB_device,
        index);

      AngularMomentum::compute( 
        i, point_count, nprim_pairs,
        s_task_data,
        pp,
        param,
        npts,
        npts,
        boys_table);
    }
    __syncthreads();
  }
}

template< template<int> class AngularMomentum, typename... Args>
void dev_integral_task_map_dispatcher(dim3 nblock, dim3 nthreads, int max_primpair, cudaStream_t stream, 
  Args&&... args) {

  // Invoke different version of the kernel based on the maximum number of primpair for this 
  // AM. The kernel with the smallest primpair buffer should perform best as it leaves the
  // most space for L1 cache. If the max number of primpairs exceeds the largest buffer, it 
  // will not use a shared memory buffer by setting primpair_limit to zero.

  // The largest buffer size is capped by the 48KB static shared memory limit; using dynamic 
  // shared memory would allow us to go higher. If the shared buffer size would exceed the 
  // limit, the use_shared to set to false to avoid a compiler error.
  if (constexpr int primpair_limit = 8; max_primpair <= primpair_limit) {
    using AM = AngularMomentum<primpair_limit>;
    task_map_kernel<AM><<<nblock, nthreads, 0, stream>>>( std::forward<Args>(args)...);

  } else if (constexpr int primpair_limit = 16; max_primpair <= primpair_limit) {
    using AM = AngularMomentum<primpair_limit>;
    task_map_kernel<AM><<<nblock, nthreads, 0, stream>>>( std::forward<Args>(args)...);

  } else if (constexpr int primpair_limit = 32; max_primpair <= primpair_limit) {
    using AM = AngularMomentum<primpair_limit>;
    task_map_kernel<AM><<<nblock, nthreads, 0, stream>>>( std::forward<Args>(args)...);

  } else {
    using AM = AngularMomentum<0>;
    task_map_kernel<AM><<<nblock, nthreads, 0, stream>>>( std::forward<Args>(args)...);
  }
}

}

